{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001",
   "metadata": {},
   "source": [
    "# Notebook 15 â€” Dataset Consolidation\n",
    "## Single `dataset/` folder Â· 60/20/20 stratified split Â· 1,342 fraud : 2,684 genuine\n",
    "\n",
    "**What this does:**\n",
    "- Pools ALL fraud images from 5 scattered locations â†’ `dataset/raw/fraud/`\n",
    "- Pools genuine images (capped at 2Ã— fraud) â†’ `dataset/raw/genuine/`\n",
    "- Moves remaining genuine to `dataset/genuine_extended/` (OCR benchmark only)\n",
    "- Applies 60/20/20 stratified split â†’ `dataset/train/`, `dataset/val/`, `dataset/test/`\n",
    "- OLD folders (`Main_Dataset/`, `Geniune Document DS/`) are kept as archive â€” NOT deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE_DIR = Path(r\"c:\\Users\\saigo\\Desktop\\fraud_document_ai\")\n",
    "DATASET_DIR = BASE_DIR / \"dataset\"\n",
    "\n",
    "# Create all required directories\n",
    "for split in [\"raw/fraud\", \"raw/genuine\", \"genuine_extended\",\n",
    "              \"train/fraud\", \"train/genuine\",\n",
    "              \"val/fraud\",   \"val/genuine\",\n",
    "              \"test/fraud\",  \"test/genuine\"]:\n",
    "    (DATASET_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 1: Collect ALL fraud image paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "FRAUD_SOURCES = [\n",
    "    BASE_DIR / \"Main_Dataset\" / \"train\"          / \"fraud\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"val\"            / \"fraud\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"test\"           / \"fraud\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"augmented\"      / \"fraud\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"augmented_debug\"/ \"fraud\",\n",
    "]\n",
    "\n",
    "all_fraud_paths = []\n",
    "for src in FRAUD_SOURCES:\n",
    "    if src.exists():\n",
    "        imgs = [p for p in src.iterdir() if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]]\n",
    "        print(f\"  {src.relative_to(BASE_DIR)}: {len(imgs)} images\")\n",
    "        all_fraud_paths.extend(imgs)\n",
    "\n",
    "print(f\"\\nðŸ“Š Total fraud images found: {len(all_fraud_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 2: Collect ALL genuine image paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "GENUINE_SOURCES = [\n",
    "    BASE_DIR / \"Main_Dataset\" / \"augmented\"      / \"genuine\",  # best quality first\n",
    "    BASE_DIR / \"Main_Dataset\" / \"augmented_debug\"/ \"genuine\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"train\"          / \"genuine\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"val\"            / \"genuine\",\n",
    "    BASE_DIR / \"Main_Dataset\" / \"test\"           / \"genuine\",\n",
    "]\n",
    "\n",
    "# Also gather from the Geniune Document DS batches\n",
    "genuine_ds = BASE_DIR / \"Geniune Document DS\"\n",
    "for batch_dir in sorted(genuine_ds.iterdir()):\n",
    "    GENUINE_SOURCES.append(batch_dir)\n",
    "    # Check for sub-sub-batches\n",
    "    for sub in batch_dir.iterdir():\n",
    "        if sub.is_dir():\n",
    "            GENUINE_SOURCES.append(sub)\n",
    "\n",
    "all_genuine_paths = []\n",
    "seen = set()\n",
    "for src in GENUINE_SOURCES:\n",
    "    if src.exists():\n",
    "        imgs = [p for p in src.iterdir() \n",
    "                if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"] and p.name not in seen]\n",
    "        for p in imgs:\n",
    "            seen.add(p.name)\n",
    "        all_genuine_paths.extend(imgs)\n",
    "\n",
    "print(f\"ðŸ“Š Total unique genuine images found: {len(all_genuine_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 3: Deduplicate fraud, cap genuine at 2Ã— fraud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Deduplicate fraud by filename\n",
    "seen_fraud = set()\n",
    "fraud_unique = []\n",
    "for p in all_fraud_paths:\n",
    "    if p.name not in seen_fraud:\n",
    "        seen_fraud.add(p.name)\n",
    "        fraud_unique.append(p)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(fraud_unique)\n",
    "random.shuffle(all_genuine_paths)\n",
    "\n",
    "n_fraud   = len(fraud_unique)\n",
    "n_genuine_cap = min(n_fraud * 2, len(all_genuine_paths))\n",
    "\n",
    "genuine_selected  = all_genuine_paths[:n_genuine_cap]\n",
    "genuine_extended  = all_genuine_paths[n_genuine_cap:]\n",
    "\n",
    "print(f\"âœ… Fraud (deduped):     {n_fraud}\")\n",
    "print(f\"âœ… Genuine (selected):  {n_genuine_cap}\")\n",
    "print(f\"ðŸ“¦ Genuine (extended):  {len(genuine_extended)} â†’ genuine_extended/\")\n",
    "print(f\"ðŸ“Š TOTAL in dataset:    {n_fraud + n_genuine_cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 4: Copy to raw/ and genuine_extended/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def safe_copy(src: Path, dest_dir: Path, prefix: str = \"\"):\n",
    "    new_name = prefix + src.name\n",
    "    dest = dest_dir / new_name\n",
    "    if dest.exists():\n",
    "        # Handle collision: append parent folder name\n",
    "        new_name = src.parent.name + \"_\" + src.name\n",
    "        dest = dest_dir / new_name\n",
    "    shutil.copy2(str(src), str(dest))\n",
    "\n",
    "print(\"Copying fraud â†’ dataset/raw/fraud/ ...\")\n",
    "for p in fraud_unique:\n",
    "    safe_copy(p, DATASET_DIR / \"raw\" / \"fraud\")\n",
    "\n",
    "print(\"Copying genuine â†’ dataset/raw/genuine/ ...\")\n",
    "for p in genuine_selected:\n",
    "    safe_copy(p, DATASET_DIR / \"raw\" / \"genuine\")\n",
    "\n",
    "print(\"Copying remaining genuine â†’ dataset/genuine_extended/ ...\")\n",
    "for p in genuine_extended:\n",
    "    safe_copy(p, DATASET_DIR / \"genuine_extended\")\n",
    "\n",
    "print(\"âœ… Copy complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 5: 60/20/20 Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "fraud_files   = sorted((DATASET_DIR / \"raw\" / \"fraud\").iterdir())\n",
    "genuine_files = sorted((DATASET_DIR / \"raw\" / \"genuine\").iterdir())\n",
    "\n",
    "all_files  = fraud_files + genuine_files\n",
    "all_labels = [0] * len(fraud_files) + [1] * len(genuine_files)  # 0=fraud, 1=genuine\n",
    "\n",
    "# First split: 60% train, 40% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    all_files, all_labels,\n",
    "    test_size=0.40, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp â†’ val, 50% â†’ test  (= 20%/20% of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} ({sum(1 for y in y_train if y==0)} fraud, {sum(1 for y in y_train if y==1)} genuine)\")\n",
    "print(f\"Val:   {len(X_val)} ({sum(1 for y in y_val if y==0)} fraud, {sum(1 for y in y_val if y==1)} genuine)\")\n",
    "print(f\"Test:  {len(X_test)} ({sum(1 for y in y_test if y==0)} fraud, {sum(1 for y in y_test if y==1)} genuine)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 6: Copy split files to train/val/test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CLASS_MAP = {0: \"fraud\", 1: \"genuine\"}\n",
    "\n",
    "def copy_split(file_list, label_list, split_name):\n",
    "    for fpath, lbl in zip(file_list, label_list):\n",
    "        dest_dir = DATASET_DIR / split_name / CLASS_MAP[lbl]\n",
    "        shutil.copy2(str(fpath), str(dest_dir / fpath.name))\n",
    "\n",
    "print(\"Writing train split...\")\n",
    "copy_split(X_train, y_train, \"train\")\n",
    "print(\"Writing val split...\")\n",
    "copy_split(X_val, y_val, \"val\")\n",
    "print(\"Writing test split...\")\n",
    "copy_split(X_test, y_test, \"test\")\n",
    "\n",
    "print(\"\\nâœ… All splits written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ STEP 7: Verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL DATASET VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total = 0\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for cls in [\"fraud\", \"genuine\"]:\n",
    "        p = DATASET_DIR / split / cls\n",
    "        n = len(list(p.iterdir()))\n",
    "        total += n\n",
    "        print(f\"  dataset/{split}/{cls}: {n} images\")\n",
    "\n",
    "ext_count = len(list((DATASET_DIR / \"genuine_extended\").iterdir()))\n",
    "print(f\"\\n  dataset/genuine_extended/: {ext_count} images (OCR benchmark only)\")\n",
    "print(f\"\\n  TOTAL in train+val+test: {total}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… Dataset consolidation complete!\")\n",
    "print(\"Old folders (Main_Dataset/, Geniune Document DS/) kept as archive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
